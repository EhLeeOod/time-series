{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "960fa6ca-b097-4cdb-ba10-37bcea1840af",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ccbe7-af0f-4174-8868-9fd450c13556",
   "metadata": {},
   "source": [
    "## Intro to pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "463c6216-d268-4db9-b0ca-4c4e87276799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pmdarima\n",
    "import pmdarima as pm\n",
    "# most important function in this package is auto_arima\n",
    "# auto_arima performs a gridsearch using specific range of values\n",
    "# still need to know whether data is seasonal and seasonal period, \"m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75978c80-692a-49f2-b231-8222f80e49f8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d6f4d9-560e-49d8-b1e7-6ac5618cbce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import statsmodels.tsa.api as tsa\n",
    "from pmdarima.model_selection import train_test_split\n",
    "from pmdarima.arima.utils import ndiffs, nsdiffs\n",
    "\n",
    "# Set wide fig size for plots\n",
    "plt.rcParams['figure.figsize']=(12,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb29ed55-9a92-483b-a01f-1a74aa423974",
   "metadata": {},
   "source": [
    "## Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "919aca9e-ced3-4000-9807-89fe1f29a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot forecast function\n",
    "def plot_forecast(ts_train, ts_test, forecast_df, n_train_lags=None, \n",
    "                  figsize=(10,4), title='Comparing Forecast vs. True Data'):\n",
    "    ### PLot training data, and forecast (with upper/,lower ci)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # setting the number of train lags to plot if not specified\n",
    "    if n_train_lags==None:\n",
    "        n_train_lags = len(ts_train)\n",
    "            \n",
    "    # Plotting Training  and test data\n",
    "    ts_train.iloc[-n_train_lags:].plot(ax=ax, label=\"train\")\n",
    "    ts_test.plot(label=\"test\", ax=ax)\n",
    "\n",
    "    # Plot forecast\n",
    "    forecast_df['mean'].plot(ax=ax, color='green', label=\"forecast\")\n",
    "\n",
    "    # Add the shaded confidence interval\n",
    "    ax.fill_between(forecast_df.index, \n",
    "                    forecast_df['mean_ci_lower'],\n",
    "                   forecast_df['mean_ci_upper'],\n",
    "                   color='green', alpha=0.3,  lw=2)\n",
    "\n",
    "    # set the title and add legend\n",
    "    ax.set_title(title)\n",
    "    ax.legend();\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c3dfa5c-c465-4428-9f96-ae4b88e36bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ad Fuller Test function\n",
    "# Custom function for Ad Fuller Test\n",
    "def get_adfuller_results(ts, alpha=.05, label='adfuller', **kwargs): #kwargs for adfuller()\n",
    "    # Saving each output\n",
    "    (test_stat, pval, nlags, nobs, crit_vals_d, \n",
    "    icbest ) = tsa.adfuller(ts, **kwargs)\n",
    "    # Converting output to a dictionary with the interpretation of p\n",
    "    adfuller_results = {'Test Statistic': test_stat,\n",
    "                        \"# of Lags Used\":nlags, \n",
    "                       '# of Observations':nobs,\n",
    "                        'p-value': round(pval,6),\n",
    "                        'alpha': alpha,\n",
    "                       'sig/stationary?': pval < alpha}\n",
    "    return pd.DataFrame(adfuller_results, index =[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85fd363c-a558-4ce7-92b3-ad363e1754bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression metrics function\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "def regression_metrics_ts(ts_true, ts_pred, label=\"\", verbose=True, output_dict=False,):\n",
    "    # Get metrics\n",
    "    mae = mean_absolute_error(ts_true, ts_pred)\n",
    "    mse = mean_squared_error(ts_true, ts_pred)\n",
    "    rmse = mean_squared_error(ts_true, ts_pred, squared=False)\n",
    "    r_squared = r2_score(ts_true, ts_pred)\n",
    "    mae_perc = mean_absolute_percentage_error(ts_true, ts_pred) * 100\n",
    "\n",
    "    if verbose == True:\n",
    "        # Print Result with label\n",
    "        header = \"---\" * 20\n",
    "        print(header, f\"Regression Metrics: {label}\", header, sep=\"\\n\")\n",
    "        print(f\"- MAE = {mae:,.3f}\")\n",
    "        print(f\"- MSE = {mse:,.3f}\")\n",
    "        print(f\"- RMSE = {rmse:,.3f}\")\n",
    "        print(f\"- R^2 = {r_squared:,.3f}\")\n",
    "        print(f\"- MAPE = {mae_perc:,.2f}%\")\n",
    "\n",
    "    if output_dict == True:\n",
    "        metrics = {\n",
    "            \"Label\": label,\n",
    "            \"MAE\": mae,\n",
    "            \"MSE\": mse,\n",
    "            \"RMSE\": rmse,\n",
    "            \"R^2\": r_squared,\n",
    "            \"MAPE(%)\": mae_perc,\n",
    "        }\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c63c5f-f061-44f5-a911-d3ed8963d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a5ae7-9930-4a2f-a6cb-c6edcc0731c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489810c-3b30-4c4a-9e6b-84c6102fd3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e953d0-8834-4130-91a8-326cefc2b0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276b730-c420-47c0-a65f-0206513e8422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
